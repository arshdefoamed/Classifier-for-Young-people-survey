{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "a9292729-8370-4fd3-9c60-5e92b215b967",
        "_uuid": "aad608c767f2f7519cd863ec831aaeb7166fed41"
      },
      "cell_type": "markdown",
      "source": "In this notebook we have the data from young people survey ([link](https://www.kaggle.com/miroslavsabo/young-people-survey)). You have joined a startup that delivers healthy meals to people. You have been tasked with doing a marketing study and understanding how likely a person is to “pay more money for good, quality or healthy food” (on a scale from 1 to 5). We will try to create a classifier for the same. The classifiers will be judged on their accuracy and precision. The reason for choosing precision is that the company would pith their product to people who are interesed in product. So high precision will ensure good return on investment."
    },
    {
      "metadata": {
        "_uuid": "b845d2006d5d0598f9fab49fd5fa527bba1cb6c2"
      },
      "cell_type": "markdown",
      "source": "We start off with basic imports required"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# some basic ibrary import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n%matplotlib inline\nimport copy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import resample\npd.set_option('display.max_columns',150)\nplt.style.use('bmh')\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint('done')",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "15980c2a-6c52-471b-bf22-c44ed8bdd618",
        "_uuid": "fd1aff49ab70cbff821d2e8c24233c0f6022cd9b"
      },
      "cell_type": "markdown",
      "source": "We load the data and remove all the rows where our target variable is empty"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# load data\nlink = 'https://www.kaggle.com/miroslavsabo/young-people-survey/downloads/responses.csv/2'\nmyDataOrig = pd.read_csv('../input/responses.csv')\nmyData = myDataOrig.copy()\ntargetVar=\"Spending on healthy eating\"\n# remove rows where targeted variable is empty\nmyData= myData.drop(myData[myData[targetVar].isnull()].index)\nprint('Done')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "0c1dda54-6fd4-4d9c-986f-19e8cd4c57be",
        "_uuid": "758c6250641b41b214a5847c3430c4a5a6111fef"
      },
      "cell_type": "markdown",
      "source": "Check whether my data is balanced or not. If not, we oversample the data to make the dstribution similar\n"
    },
    {
      "metadata": {
        "_cell_guid": "fac78340-a971-4e40-8110-d52e7c0d44e8",
        "_uuid": "815edb61514609de4ad10bba65416056e9ffba05",
        "trusted": true
      },
      "cell_type": "code",
      "source": "myData[targetVar].value_counts().sort_index()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "1.0     41\n2.0    132\n3.0    282\n4.0    330\n5.0    223\nName: Spending on healthy eating, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79efe40c-a4c8-4ff5-88be-e8d5827ed6e2",
        "_uuid": "e1fe8f77ee42ae8cdd3a6ca13bc5d0ba3010b53c"
      },
      "cell_type": "markdown",
      "source": "Turns out the data is highly imbalanced, we oversample class 1 and class 2 to 220 examples"
    },
    {
      "metadata": {
        "_cell_guid": "520416f3-e6a4-4487-9f26-e7deac4eacef",
        "_uuid": "a30d881ff1d326bf57ca96e7430b160c6b720d14",
        "trusted": true
      },
      "cell_type": "code",
      "source": "minorityData= myData[myData[targetVar]==1]\nminorityData2= myData[myData[targetVar]==2]\nmajorityData1 =myData[myData[targetVar] ==3]\nmajorityData2 =myData[myData[targetVar] ==4]\nmajorityData3 =myData[myData[targetVar] ==5]\nupsampledMinorityData = resample(minorityData,replace=True,n_samples=220,random_state=123)\nupsampledMinorityData2 = resample(minorityData2,replace=True,n_samples=220,random_state=123)\nmyData=pd.concat([majorityData1,majorityData2,majorityData3,upsampledMinorityData,upsampledMinorityData2])\nmyData[targetVar].value_counts().sort_index()\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "1.0    220\n2.0    220\n3.0    282\n4.0    330\n5.0    223\nName: Spending on healthy eating, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "30ff77ac-60d7-40ed-98b9-1db8d0766f18",
        "_uuid": "023eb28be748a2e705f129316c6daf0639ba6209"
      },
      "cell_type": "markdown",
      "source": "Most of the data is from range 1 to 5. So we convert it into categorical data type. Also, we replace the missing values in those features with the most frequent category.\nThe features that are non-categorical are 'Age','Height','Weight','Number of siblings'. We replace their missing values with the rounded off means of the data"
    },
    {
      "metadata": {
        "_cell_guid": "7782056e-cc40-4ee7-92e0-876a222a49d8",
        "_uuid": "7fb3de7724923aeb2c259187a56327fa763fafbb",
        "trusted": true
      },
      "cell_type": "code",
      "source": "for colName in myData.columns.difference(['Age','Height','Weight','Number of siblings']):        \n    myData[colName]=myData[colName].astype('category')\n    modVal=myData[colName].describe().top\n    myData[colName][myData[colName].isnull()]=modVal\n    \nfor colName in myData[['Age','Height','Weight','Number of siblings']].columns:\n    meanVal= round((myData[colName].describe())[1])\n    myData[colName][myData[colName].isnull()]=meanVal\nprint('Done')",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b2d22cf4-6136-4490-a69c-d85b42622f72",
        "_uuid": "73728080b1d25791e39c081bc6a3aca711385bea"
      },
      "cell_type": "markdown",
      "source": "Our data cleaning is done. We load sklearn library modules for processing needed later on"
    },
    {
      "metadata": {
        "_cell_guid": "f631044c-f4c1-4d23-a7d8-da085502e9d3",
        "_uuid": "116e1b419c44eabb5bf22603cfb6126478addb61",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nprint('Done')",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "5b558c36-9135-4eeb-84c7-d9accd082dfa",
        "_uuid": "1420b1f998f72493ca7901fd83ac3a4145c76eaf",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "We create dummy features for categorical data. This will result in large number of features. We choose top 150 features to avoid problems because of high dimensional data.  Then we split the data into training set, development set and test set. The data is divided into 70:10:20 ratio"
    },
    {
      "metadata": {
        "_cell_guid": "a00687c5-f22e-4bfb-9128-6bd572e6637e",
        "_uuid": "2235cd845fc4b3a6c1f42fa934548b005b42e11f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "RANDOM_STATE = 3812 #random state to produce consistent results. 3812 is randomly chosen number here\nfeatures= myData[myData.columns.difference([targetVar])]\nfeatures = pd.get_dummies(features)\ntarget = myData[targetVar]\nX_new = SelectKBest(chi2, k=150).fit_transform(features, target)\nX_train, X_test, y_train, y_test = train_test_split(X_new, target,test_size=0.20,random_state =RANDOM_STATE)\nX_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train,test_size=0.125,random_state =RANDOM_STATE)\n\nprint('Done')",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "17c5b7a1-9cf2-443f-8a41-3f111675c071",
        "_uuid": "8a6d00353dd547c2c7316dc2a6b094a110494abd"
      },
      "cell_type": "markdown",
      "source": "The code below is a function to get development accuracy for a given classifier"
    },
    {
      "metadata": {
        "_cell_guid": "f5dcbafd-30ee-4e63-9583-49ff598bcc49",
        "_uuid": "2d62b0811bbbbe0838ca4f8c9be3df95ad6e3695",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def getDevAccuracyForGivenClassifier(classifier):\n    boostedClassifier = make_pipeline(StandardScaler(),classifier)\n    boostedClassifier.fit(X_train, y_train)\n    pred_Dev_Data = boostedClassifier.predict(X_dev)\n    devAccScore = metrics.accuracy_score(y_dev, pred_Dev_Data)\n    return devAccScore",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "19c5e347-748e-4af3-9ee2-03c6ac184487",
        "_uuid": "4382b06b46ac5637b356d167afdfcbe722bfc5c0"
      },
      "cell_type": "markdown",
      "source": "Here we tune paramaters of different classifiers based on their performance on development set. The classifiers used are: Random Forest, Decision Tree, Support Vector Machine, Neural Networks, Adaboost classifier, Bagging Classifier"
    },
    {
      "metadata": {
        "_cell_guid": "1c5cb30a-5600-4460-848e-754fc37495d5",
        "_uuid": "b201beb28f361663eacb273530e01f63889a6b74",
        "trusted": true
      },
      "cell_type": "code",
      "source": "depthArr = [1,2,5,10,15,20]\nbestRFdepth=0\nbestAcc=0\nfor i in range(len(depthArr)):\n    randomForest_clf = RandomForestClassifier(n_jobs=100,random_state=RANDOM_STATE, max_depth=depthArr[i])\n    devAccScore=getDevAccuracyForGivenClassifier(randomForest_clf)\n#     print(devAccScore)\n    if devAccScore>bestAcc:\n        bestAcc=devAccScore\n        bestRFdepth=depthArr[i]\nprint(\"Best depth for random forest is: \",end=\"\");print(bestRFdepth)\n\ndepthArr = [1,2,5,10,15,20]\nbestDTdepth=0\nbestAcc=0\nfor i in range(len(depthArr)):\n    decisionTree_clf = DecisionTreeClassifier(max_depth=depthArr[i],random_state=RANDOM_STATE)\n    devAccScore=getDevAccuracyForGivenClassifier(decisionTree_clf)\n#     print(devAccScore)\n    if devAccScore>bestAcc:\n        bestAcc=devAccScore\n        bestDTdepth=depthArr[i]\nprint(\"Best depth for DT is: \",end=\"\");print(bestDTdepth)\n\ngammaArr = [0.001,0.01,0.1,0.2,0.5,1,2,5,10]\nbestSVMGamma=0\nbestAcc=0\nfor i in range(len(gammaArr)):\n    svc_clf=SVC(gamma=gammaArr[i],random_state=RANDOM_STATE)\n    devAccScore=getDevAccuracyForGivenClassifier(svc_clf)\n#     print(devAccScore)\n    if devAccScore>bestAcc:\n        bestAcc=devAccScore\n        bestSVMGamma=gammaArr[i]\nprint(\"Best gamma for SVM is: \",end=\"\");print(bestSVMGamma)\n\nhiddenLaterCtArr = [1,2,5,10,20,50,100,200,500]\nbestNumberOfHiddenLayers=0\nbestAcc=0\nfor i in range(len(hiddenLaterCtArr)):\n    neuralNet_clf=MLPClassifier(hidden_layer_sizes=(hiddenLaterCtArr[i],),random_state=RANDOM_STATE)\n    devAccScore=getDevAccuracyForGivenClassifier(neuralNet_clf)\n#     print(devAccScore)\n    if devAccScore>bestAcc:\n        bestAcc=devAccScore\n        bestNumberOfHiddenLayers=hiddenLaterCtArr[i]\nprint(\"Best number of hidden layers for neural networks is: \",end=\"\");print(bestNumberOfHiddenLayers)\n\nestimatorCtArr = [10,20,50,100,200,500,1000,2000]\nbestNumberOfEstimators=0\nbestAcc=0\nfor i in range(len(estimatorCtArr)):\n    adaBoostCLF=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=bestDTdepth,random_state=RANDOM_STATE),n_estimators =estimatorCtArr[i])\n    devAccScore=getDevAccuracyForGivenClassifier(adaBoostCLF)\n#     print(devAccScore)\n    if devAccScore>bestAcc:\n        bestAcc=devAccScore\n        bestNumberOfEstimators=estimatorCtArr[i]\nprint(\"Best number of estimators for ada boost is: \",end=\"\");print(bestNumberOfEstimators)\n\nestimatorCtArr = [10,20,50,100,200,500,1000,2000]\nbestNumberOfBaggingEstimators=0\nbestAcc=0\nfor i in range(len(estimatorCtArr)):\n    bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=bestDTdepth,random_state=RANDOM_STATE), n_estimators=estimatorCtArr[i],bootstrap=True, n_jobs=-1, oob_score=True, random_state=RANDOM_STATE)\n    devAccScore=getDevAccuracyForGivenClassifier(bag_clf)\n#     print(devAccScore)\n    if devAccScore>bestAcc:\n        bestAcc=devAccScore\n        bestNumberOfBaggingEstimators=estimatorCtArr[i]\nprint(\"Best number of estimators for bagging classifier is: \",end=\"\");print(bestNumberOfBaggingEstimators)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Best depth for random forest is: 15\nBest depth for DT is: 15\nBest gamma for SVM is: 0.01\nBest number of hidden layers for neural networks is: 200\nBest number of estimators for ada boost is: 2000\nBest number of estimators for bagging classifier is: 50\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "390e99f0-7256-437e-8ca6-d538c23c777f",
        "_uuid": "a454ac586990186e0f87f7d16645c874de0df150"
      },
      "cell_type": "markdown",
      "source": "Here we train the classifiers based on their optimal hyperparameters and apply them on the test data. Then, we manually compare their accuracy, f1-score and classification report"
    },
    {
      "metadata": {
        "_cell_guid": "8573e3d7-0b02-4ff9-9f00-1240a65a27dd",
        "_uuid": "0e62e2a476e0d61198746489058ba12cfcb9e2b1",
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "randomForest_clf = RandomForestClassifier(n_jobs=200,random_state=RANDOM_STATE, max_depth=bestRFdepth)\ndecisionTree_clf = DecisionTreeClassifier(max_depth=bestDTdepth,random_state=RANDOM_STATE)\ngaussianNB_clf = GaussianNB()\nsvc_clf=SVC(gamma=bestSVMGamma,random_state=RANDOM_STATE)\nneuralNet_clf=MLPClassifier(hidden_layer_sizes=(bestNumberOfHiddenLayers,),random_state=RANDOM_STATE)\ncombinationClassifier=VotingClassifier(estimators=[('randFor',randomForest_clf),('svc',svc_clf),('neuralNet_clf',neuralNet_clf)])\nadaBoostCLF=AdaBoostClassifier(base_estimator=decisionTree_clf,n_estimators =bestNumberOfEstimators)\nbag_clf = BaggingClassifier(decisionTree_clf, n_estimators=bestNumberOfBaggingEstimators,bootstrap=True, n_jobs=-1, oob_score=True, random_state=RANDOM_STATE)\n\nclassifierArr=[randomForest_clf,decisionTree_clf,gaussianNB_clf,svc_clf,neuralNet_clf,combinationClassifier,adaBoostCLF,bag_clf]\nclassifierName=['Random Forest','Decision Tree','Gaussian NB','Support Vector','Neural Network', 'Voting Classifier','adaBoost Classifier','Bagging Classifier']\nclassifierNum = -1\nprint('---------------------------------------------------------------- \\n')\nfor i in range(len(classifierArr)):\n    if(classifierNum != -1):\n        i= classifierNum\n    boostedClassifier = make_pipeline(StandardScaler(),classifierArr[i])\n    boostedClassifier.fit(X_train, y_train)\n    pred_train_std = boostedClassifier.predict(X_train)\n    pred_test_std = boostedClassifier.predict(X_test)\n    print('----------------------------------'+classifierName[i]+'-------------------------')\n    print('Prediction accuracy for the training dataset:',end = \" \")\n    print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n    print('Prediction accuracy for the test dataset:',end = \" \")\n    print('{:.2%} \\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n    print('f1 score is: {:.2}'.format(f1_score(y_test, pred_test_std,average='macro')))\n    print(classification_report(y_test, pred_test_std))\n    if(classifierNum != -1):\n        break;\nprint('\\n ----------------------------------XXXXXX-------------------------')",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "---------------------------------------------------------------- \n\n----------------------------------Random Forest-------------------------\nPrediction accuracy for the training dataset: 97.98%\nPrediction accuracy for the test dataset: 49.02% \n\nf1 score is: 0.51\n             precision    recall  f1-score   support\n\n        1.0       0.73      1.00      0.85        33\n        2.0       0.64      0.64      0.64        45\n        3.0       0.35      0.33      0.34        58\n        4.0       0.41      0.44      0.42        71\n        5.0       0.37      0.27      0.31        48\n\navg / total       0.47      0.49      0.48       255\n\n----------------------------------Decision Tree-------------------------\nPrediction accuracy for the training dataset: 97.20%\nPrediction accuracy for the test dataset: 44.31% \n\nf1 score is: 0.49\n             precision    recall  f1-score   support\n\n        1.0       0.89      1.00      0.94        33\n        2.0       0.58      0.64      0.61        45\n        3.0       0.30      0.28      0.29        58\n        4.0       0.30      0.28      0.29        71\n        5.0       0.31      0.31      0.31        48\n\navg / total       0.43      0.44      0.43       255\n\n----------------------------------Gaussian NB-------------------------\nPrediction accuracy for the training dataset: 47.31%\nPrediction accuracy for the test dataset: 35.29% \n\nf1 score is: 0.36\n             precision    recall  f1-score   support\n\n        1.0       0.65      0.73      0.69        33\n        2.0       0.27      0.78      0.40        45\n        3.0       0.22      0.22      0.22        58\n        4.0       0.71      0.07      0.13        71\n        5.0       0.57      0.27      0.37        48\n\navg / total       0.49      0.35      0.31       255\n\n----------------------------------Support Vector-------------------------\nPrediction accuracy for the training dataset: 98.43%\nPrediction accuracy for the test dataset: 56.86% \n\nf1 score is: 0.61\n             precision    recall  f1-score   support\n\n        1.0       0.97      0.94      0.95        33\n        2.0       0.93      0.60      0.73        45\n        3.0       0.49      0.38      0.43        58\n        4.0       0.42      0.68      0.52        71\n        5.0       0.47      0.35      0.40        48\n\navg / total       0.61      0.57      0.57       255\n\n----------------------------------Neural Network-------------------------\nPrediction accuracy for the training dataset: 100.00%\nPrediction accuracy for the test dataset: 50.59% \n\nf1 score is: 0.54\n             precision    recall  f1-score   support\n\n        1.0       0.80      1.00      0.89        33\n        2.0       0.67      0.64      0.66        45\n        3.0       0.38      0.38      0.38        58\n        4.0       0.38      0.41      0.39        71\n        5.0       0.44      0.33      0.38        48\n\navg / total       0.50      0.51      0.50       255\n\n----------------------------------Voting Classifier-------------------------\nPrediction accuracy for the training dataset: 99.89%\nPrediction accuracy for the test dataset: 53.33% \n\nf1 score is: 0.57\n             precision    recall  f1-score   support\n\n        1.0       0.82      1.00      0.90        33\n        2.0       0.69      0.64      0.67        45\n        3.0       0.40      0.36      0.38        58\n        4.0       0.41      0.51      0.45        71\n        5.0       0.52      0.35      0.42        48\n\navg / total       0.53      0.53      0.53       255\n\n----------------------------------adaBoost Classifier-------------------------\nPrediction accuracy for the training dataset: 100.00%\nPrediction accuracy for the test dataset: 54.90% \n\nf1 score is: 0.59\n             precision    recall  f1-score   support\n\n        1.0       0.97      1.00      0.99        33\n        2.0       0.97      0.62      0.76        45\n        3.0       0.36      0.33      0.34        58\n        4.0       0.41      0.66      0.51        71\n        5.0       0.54      0.27      0.36        48\n\navg / total       0.59      0.55      0.55       255\n\n----------------------------------Bagging Classifier-------------------------\nPrediction accuracy for the training dataset: 100.00%\nPrediction accuracy for the test dataset: 49.41% \n\nf1 score is: 0.54\n             precision    recall  f1-score   support\n\n        1.0       0.85      1.00      0.92        33\n        2.0       0.74      0.71      0.73        45\n        3.0       0.29      0.34      0.31        58\n        4.0       0.34      0.32      0.33        71\n        5.0       0.50      0.38      0.43        48\n\navg / total       0.50      0.49      0.49       255\n\n\n ----------------------------------XXXXXX-------------------------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "52660c5e-e2b5-460f-b658-37954567c437",
        "_kg_hide-output": true,
        "_uuid": "e9d3cbf095ababf2113c34772ee1449448dc8fa7"
      },
      "cell_type": "markdown",
      "source": "As we can see, support vecotr machine has the best  test accuracy. Also, the precision is highest. Hence it is the best classifier for our problem statement. Let's observe the performance on validation data again and observet the values where there was a mismatch"
    },
    {
      "metadata": {
        "_cell_guid": "d22c029e-646e-46ab-aa5f-22d7c6c9da50",
        "_uuid": "fa8a62740365a7daaae890555d687964968b87a7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "svc_clf=SVC(gamma=bestSVMGamma,random_state=RANDOM_STATE,decision_function_shape='ovr')\nsvc_clf.fit(X_train, y_train)\npred_Dev_Data = svc_clf.predict(X_dev)\n\n(y_dev==pred_Dev_Data).head()\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "845    False\n627     True\n928    False\n395     True\n374    False\nName: Spending on healthy eating, dtype: bool"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "10756b7bc7b29af6fa91e3f51a6e06d4629b2070"
      },
      "cell_type": "markdown",
      "source": "Now we observe the cases of feature vector where there was correct classification"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "766272d4d3d30a09eed25d797d47eb3da0e17bcd"
      },
      "cell_type": "code",
      "source": "# X_dev[0]\npredictedVal = svc_clf.predict([X_dev[1]])\nprint(predictedVal[0])\nprint(y_dev.values[1])\nprint(svc_clf.decision_function(X_dev)[1])",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1.0\n1.0\n[ 4.32247471  0.90521899  1.89505941  3.00988481 -0.13263791]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "cee86b23c603ee0957c5ab4ea5daea20c96e2733"
      },
      "cell_type": "markdown",
      "source": "We can observe that index for which decision function value is highest is chosen, Now lets try to consider the case where there was misclassification"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66ef06f30c210866d5849d64b5da887553c68ffd"
      },
      "cell_type": "code",
      "source": "predictedVal = svc_clf.predict([X_dev[2]])\nprint(predictedVal[0])\nprint(y_dev.values[2])\nprint(svc_clf.decision_function(X_dev)[2])",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "4.0\n1.0\n[ 3.10977236 -0.16589295  1.9756115   4.20985883  0.87065027]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "245cd7be1900f8e91ebeccc0c3eb853b5be6903b"
      },
      "cell_type": "markdown",
      "source": "In the above case , the score of label 4 is higher than score of label 1. We can improve the score by increasing/decreasing the gamma score. lets try decreeasing it."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23cca22a81a6e660bf018a401626b9ea515469ff"
      },
      "cell_type": "code",
      "source": "svc_clf=SVC(gamma=bestSVMGamma/2,random_state=RANDOM_STATE,decision_function_shape='ovr')\nsvc_clf.fit(X_train, y_train)\npredictedVal = svc_clf.predict([X_dev[2]])\nprint(predictedVal[0])\nprint(y_dev.values[2])\nprint(svc_clf.decision_function(X_dev)[2])\n",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "4.0\n1.0\n[ 3.07548494 -0.18592052  1.98344413  4.23680872  0.89018273]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "6de64d8cce36e380e49009f1d254c7ee203d1dca"
      },
      "cell_type": "markdown",
      "source": "The score for label 1 decreased. So decreasing gamma was a bad choice. Lets try to increase it and check the values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e3a99b11e98007e94291ee27eac5e14d4053509"
      },
      "cell_type": "code",
      "source": "svc_clf=SVC(gamma=bestSVMGamma*2,random_state=RANDOM_STATE,decision_function_shape='ovr')\nsvc_clf.fit(X_train, y_train)\npredictedVal = svc_clf.predict([X_dev[2]])\nprint(predictedVal[0])\nprint(y_dev.values[2])\nprint(svc_clf.decision_function(X_dev)[2])",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1.0\n1.0\n[ 4.24287492 -0.20501092  1.95567784  3.16421632  0.84224184]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "effee4eaf4624d2c433ca68e4dc8475722ac49b4"
      },
      "cell_type": "markdown",
      "source": "By increasing gamma, the given example is rightly predicted now. We can do this for whole development data to get idea of how much to increase/decrease gamma to improve the classifier"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}